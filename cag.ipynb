{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd88dd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x11b205a90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11b206510>, root_client=<openai.OpenAI object at 0x11a7a30e0>, root_async_client=<openai.AsyncOpenAI object at 0x11b206270>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm=init_chat_model(\"openai:gpt-4o-mini\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfeb418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cache variable\n",
    "Model_Cache={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be1cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def cache_model(query):\n",
    "    start_time=time.time()\n",
    "    if Model_Cache.get(query):\n",
    "        print(\"**CAche Hit**\")\n",
    "        end_time=time.time()\n",
    "        elapsed_time=end_time-start_time\n",
    "        print(f\"EXECUTION TIME: {elapsed_time:.2f} seconds\")\n",
    "        return Model_Cache.get(query)\n",
    "    else:\n",
    "        print(\"***CACHE MISS – EXECUTING MODEL***\")\n",
    "        start_time = time.time()\n",
    "        response = llm.invoke(query)\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"EXECUTION TIME: {elapsed:.2f} seconds\")\n",
    "        Model_Cache[query] = response\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fedc3833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 3.21 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_11f3029f6b', 'id': 'chatcmpl-CmwAK4T8E0AE72ROJTlqBDmLjRCq6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--541702e3-af1b-4b49-b236-bb16eb194229-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=cache_model(\"hi\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c76ce20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hi': AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_11f3029f6b', 'id': 'chatcmpl-CmwAK4T8E0AE72ROJTlqBDmLjRCq6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--541702e3-af1b-4b49-b236-bb16eb194229-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "209658a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**CAche Hit**\n",
      "EXECUTION TIME: 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_11f3029f6b', 'id': 'chatcmpl-CmwAK4T8E0AE72ROJTlqBDmLjRCq6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--541702e3-af1b-4b49-b236-bb16eb194229-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=cache_model(\"hi\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18427c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 20.02 seconds\n",
      "content=\"LangGraph is an innovative platform designed to enhance the way we engage with language learning and understanding. Emerging from the intersection of linguistics, technology, and artificial intelligence, LangGraph harnesses advanced computational techniques to create a dynamic and interactive experience for users of all ages and backgrounds.\\n\\nAt its core, LangGraph operates on the principle that language is not just a system of grammar and vocabulary, but a vast network of connections and contexts. By visualizing these relationships, LangGraph allows learners to see language structures and meanings in a more holistic way. This graphical representation aids users in understanding how words and phrases interconnect, providing insights that traditional learning methods might overlook.\\n\\nOne of the key features of LangGraph is its user-friendly interface, which incorporates interactive elements that encourage exploration and discovery. Users can click on words, phrases, or concepts and view their relationships with other elements within the language. For instance, clicking on a verb could display its various forms, related nouns, adjectives, and even contextual sentences demonstrating its use in different situations. This interactivity not only makes learning more engaging but also fosters a deeper comprehension of the language in context.\\n\\nMoreover, LangGraph employs sophisticated algorithms that adapt to the user's progression. As learners interact with the platform, it analyzes their usage patterns, vocabulary acquisition, and areas of difficulty. This data-driven approach allows the system to personalize the learning experience, recommending specific resources, exercises, and content tailored to the user’s individual needs. Such adaptation is particularly beneficial for learners at various proficiency levels, as it ensures that content remains challenging yet achievable.\\n\\nAdditionally, LangGraph offers a collaborative dimension to language learning. Users can connect with peers, instructors, and language enthusiasts around the world, facilitating a community-driven approach to practice and feedback. This social element is crucial, as language is inherently a communicative tool, and engaging with others can significantly enhance fluency and confidence. Through forums, discussion boards, and collaborative projects, users can practice real-world language usage and gain cultural insights that enrich their understanding.\\n\\nAnother notable aspect of LangGraph is its incorporation of gamification principles. By integrating game-like elements, such as rewards, challenges, and progress tracking, the platform motivates learners to consistently engage with the material. This approach helps transform the often daunting task of language learning into a more enjoyable and rewarding endeavor, ultimately sustaining users' interest and commitment over time.\\n\\nFurthermore, LangGraph is adaptable to various learning contexts, whether for academic purposes, professional development, or personal enrichment. Its versatility makes it suitable for schools, corporate training programs, and individual learners alike. Institutions can utilize LangGraph as a supplemental tool to reinforce curriculum, while students can benefit from its vast resources outside the classroom environment.\\n\\nIn conclusion, LangGraph represents a significant advancement in language learning technology. By combining interactive graphical representations, personalized learning experiences, community engagement, and gamification, it sets a new standard for how languages can be taught and learned. As globalization continues to connect people across cultures, tools like LangGraph will play an essential role in fostering communication and understanding through language, equipping learners with the skills necessary to thrive in a multilingual world. Whether you are a beginner taking your first steps into a new language or an advanced learner seeking to refine your skills, LangGraph offers a comprehensive, engaging, and effective pathway to language mastery.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 661, 'prompt_tokens': 18, 'total_tokens': 679, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_11f3029f6b', 'id': 'chatcmpl-CmwAn4CJuoXPoC6XSWbWgQL73dKdQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--cfc32163-c297-471a-80b5-e20f1f92d226-0' usage_metadata={'input_tokens': 18, 'output_tokens': 661, 'total_tokens': 679, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "query=\"can you give me 500 words on langgraph?\"\n",
    "response =cache_model(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93428ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**CAche Hit**\n",
      "EXECUTION TIME: 0.00 seconds\n",
      "content=\"LangGraph is an innovative platform designed to enhance the way we engage with language learning and understanding. Emerging from the intersection of linguistics, technology, and artificial intelligence, LangGraph harnesses advanced computational techniques to create a dynamic and interactive experience for users of all ages and backgrounds.\\n\\nAt its core, LangGraph operates on the principle that language is not just a system of grammar and vocabulary, but a vast network of connections and contexts. By visualizing these relationships, LangGraph allows learners to see language structures and meanings in a more holistic way. This graphical representation aids users in understanding how words and phrases interconnect, providing insights that traditional learning methods might overlook.\\n\\nOne of the key features of LangGraph is its user-friendly interface, which incorporates interactive elements that encourage exploration and discovery. Users can click on words, phrases, or concepts and view their relationships with other elements within the language. For instance, clicking on a verb could display its various forms, related nouns, adjectives, and even contextual sentences demonstrating its use in different situations. This interactivity not only makes learning more engaging but also fosters a deeper comprehension of the language in context.\\n\\nMoreover, LangGraph employs sophisticated algorithms that adapt to the user's progression. As learners interact with the platform, it analyzes their usage patterns, vocabulary acquisition, and areas of difficulty. This data-driven approach allows the system to personalize the learning experience, recommending specific resources, exercises, and content tailored to the user’s individual needs. Such adaptation is particularly beneficial for learners at various proficiency levels, as it ensures that content remains challenging yet achievable.\\n\\nAdditionally, LangGraph offers a collaborative dimension to language learning. Users can connect with peers, instructors, and language enthusiasts around the world, facilitating a community-driven approach to practice and feedback. This social element is crucial, as language is inherently a communicative tool, and engaging with others can significantly enhance fluency and confidence. Through forums, discussion boards, and collaborative projects, users can practice real-world language usage and gain cultural insights that enrich their understanding.\\n\\nAnother notable aspect of LangGraph is its incorporation of gamification principles. By integrating game-like elements, such as rewards, challenges, and progress tracking, the platform motivates learners to consistently engage with the material. This approach helps transform the often daunting task of language learning into a more enjoyable and rewarding endeavor, ultimately sustaining users' interest and commitment over time.\\n\\nFurthermore, LangGraph is adaptable to various learning contexts, whether for academic purposes, professional development, or personal enrichment. Its versatility makes it suitable for schools, corporate training programs, and individual learners alike. Institutions can utilize LangGraph as a supplemental tool to reinforce curriculum, while students can benefit from its vast resources outside the classroom environment.\\n\\nIn conclusion, LangGraph represents a significant advancement in language learning technology. By combining interactive graphical representations, personalized learning experiences, community engagement, and gamification, it sets a new standard for how languages can be taught and learned. As globalization continues to connect people across cultures, tools like LangGraph will play an essential role in fostering communication and understanding through language, equipping learners with the skills necessary to thrive in a multilingual world. Whether you are a beginner taking your first steps into a new language or an advanced learner seeking to refine your skills, LangGraph offers a comprehensive, engaging, and effective pathway to language mastery.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 661, 'prompt_tokens': 18, 'total_tokens': 679, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_11f3029f6b', 'id': 'chatcmpl-CmwAn4CJuoXPoC6XSWbWgQL73dKdQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--cfc32163-c297-471a-80b5-e20f1f92d226-0' usage_metadata={'input_tokens': 18, 'output_tokens': 661, 'total_tokens': 679, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "query=\"can you give me 500 words on langgraph?\"\n",
    "response =cache_model(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985e788",
   "metadata": {},
   "source": [
    "ADVANCE  CAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdfc93b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable forward references for type hints (allows using types before they're defined)\n",
    "from __future__ import annotations\n",
    "# Type hints for defining structured data types and collections\n",
    "from typing import TypedDict, List, Optional\n",
    "# For measuring execution time and performance metrics\n",
    "import time\n",
    "\n",
    "# ---- LangGraph / LangChain ----\n",
    "# StateGraph: Build stateful, multi-step agent workflows\n",
    "# END: Terminal node to end the graph execution\n",
    "from langgraph.graph import StateGraph, END\n",
    "# MemorySaver: Persist conversation state and checkpoints in memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Document: Core data structure for storing text chunks with metadata\n",
    "from langchain_core.documents import Document\n",
    "# ChatOpenAI: OpenAI's chat model interface for LLM interactions\n",
    "from langchain_openai import ChatOpenAI\n",
    "# HuggingFaceEmbeddings: Generate embeddings using HuggingFace models\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ---- FAISS vector stores ----\n",
    "# FAISS: Facebook AI Similarity Search library for efficient vector similarity search\n",
    "import faiss\n",
    "# FAISS: LangChain wrapper for FAISS vector store operations\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# InMemoryDocstore: In-memory document storage for FAISS index\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74535796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18660024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= CONFIG =================\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-dim\n",
    "VECTOR_DIM = 384\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "LLM_TEMPERATURE = 0\n",
    "\n",
    "RETRIEVE_TOP_K = 4\n",
    "CACHE_TOP_K = 3\n",
    "\n",
    "CACHE_DISTANCE_THRESHOLD = 0.45 #semantic similarity threshold \n",
    "\n",
    "# Optional TTL for cache entries (seconds). 0 = disabled.\n",
    "CACHE_TTL_SEC = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ada7e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= STATE ==================\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    normalized_question: str\n",
    "    context_docs: List[Document]\n",
    "    answer: Optional[str]\n",
    "    citations: List[str]\n",
    "    cache_hit: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cc8e5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nischitha/Documents/Agentic-RAG/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============== GLOBALS ===================\n",
    "EMBED = HuggingFaceEmbeddings(model_name=EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ec270cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#respond from cache if the question is same\n",
    "# ----- QA CACHE (EMPTY, SAFE INIT) -----\n",
    "qa_index = faiss.IndexFlatL2(VECTOR_DIM)  # distance; lower is better\n",
    "QA_CACHE = FAISS(\n",
    "    embedding_function=EMBED,\n",
    "    index=qa_index,\n",
    "    docstore=InMemoryDocstore({}),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- QA CACHE (EMPTY, SAFE INIT) -----\n",
    "qa_index = faiss.IndexFlatL2(VECTOR_DIM)  # distance; lower is better\n",
    "QA_CACHE = FAISS(\n",
    "    embedding_function=EMBED,\n",
    "    index=qa_index,\n",
    "    docstore=InMemoryDocstore({}),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de3706bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x179800440>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b99ade6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----- RAG STORE (demo only) -----\n",
    "# RAG_STORE = FAISS.from_texts(\n",
    "#     texts=[\n",
    "#         \"LangGraph lets you compose stateful LLM workflows as graphs.\",\n",
    "#         \"In LangGraph, nodes can be cached; node caching memoizes outputs keyed by inputs for a TTL.\",\n",
    "#         \"Retrieval-Augmented Generation (RAG) retrieves external context and injects it into prompts.\",\n",
    "#         \"Semantic caching reuses prior answers when new questions are semantically similar.\"\n",
    "#     ],\n",
    "#     embedding=EMBED,\n",
    "# )\n",
    "\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorstore\n",
    "RAG_STORE=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=EMBED\n",
    ")\n",
    "\n",
    "\n",
    "#retriever=RAG_STORE.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "035993fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = ChatOpenAI(model=LLM_MODEL, temperature=LLM_TEMPERATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "111eb661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x3135ac2d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x3135ad810>, root_client=<openai.OpenAI object at 0x3135acb90>, root_async_client=<openai.AsyncOpenAI object at 0x3135acf50>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1612f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ NODES ===================\n",
    "def normalize_query(state: RAGState) -> RAGState:\n",
    "    q = (state[\"question\"] or \"\").strip()\n",
    "    state[\"normalized_question\"] = q.lower()\n",
    "    return state\n",
    "\n",
    "def semantic_cache_lookup(state: RAGState) -> RAGState:\n",
    "    q = state[\"normalized_question\"]\n",
    "    state[\"cache_hit\"] = False  # default\n",
    "\n",
    "    if not q:\n",
    "        return state\n",
    "\n",
    "    # ✅ Guard: FAISS crashes if ntotal == 0 and you ask for k>0\n",
    "    if getattr(QA_CACHE, \"index\", None) is None or QA_CACHE.index.ntotal == 0:\n",
    "        return state\n",
    "\n",
    "    # For FAISS L2 wrapper, this returns (Document, distance) with lower=better\n",
    "    hits = QA_CACHE.similarity_search_with_score(q, k=CACHE_TOP_K)\n",
    "    if not hits:\n",
    "        return state\n",
    "\n",
    "    best_doc, dist = hits[0]\n",
    "\n",
    "    # Optional TTL\n",
    "    if CACHE_TTL_SEC > 0:\n",
    "        ts = best_doc.metadata.get(\"ts\")\n",
    "        if ts is None or (time.time() - float(ts)) > CACHE_TTL_SEC:\n",
    "            return state\n",
    "\n",
    "    # L2 distance gate (lower = more similar)\n",
    "    if dist <= CACHE_DISTANCE_THRESHOLD:\n",
    "        cached_answer = best_doc.metadata.get(\"answer\")\n",
    "        if cached_answer:\n",
    "            state[\"answer\"] = cached_answer\n",
    "            state[\"citations\"] = [\"(cache)\"]\n",
    "            state[\"cache_hit\"] = True\n",
    "\n",
    "    return state\n",
    "\n",
    "def respond_from_cache(state: RAGState) -> RAGState:\n",
    "    return state\n",
    "\n",
    "def retrieve(state: RAGState) -> RAGState:\n",
    "    q = state[\"normalized_question\"]\n",
    "    docs = RAG_STORE.similarity_search(q, k=RETRIEVE_TOP_K)\n",
    "    state[\"context_docs\"] = docs\n",
    "    return state\n",
    "\n",
    "def generate(state: RAGState) -> RAGState:\n",
    "    q = state[\"question\"]\n",
    "    docs = state.get(\"context_docs\", [])\n",
    "    ctx = \"\\n\\n\".join([f\"[doc-{i}] {d.page_content}\" for i, d in enumerate(docs, start=1)])\n",
    "\n",
    "    system = (\n",
    "        \"You are a precise RAG assistant. Use the context when helpful. \"\n",
    "        \"Cite with [doc-i] markers if you use a fact from the context.\"\n",
    "    )\n",
    "    user = f\"Question: {q}\\n\\nContext:\\n{ctx}\\n\\nWrite a concise answer with citations.\"\n",
    "\n",
    "    resp = LLM.invoke([{\"role\": \"system\", \"content\": system},\n",
    "                       {\"role\": \"user\", \"content\": user}])\n",
    "    state[\"answer\"] = resp.content\n",
    "    state[\"citations\"] = [f\"[doc-{i}]\" for i in range(1, len(docs) + 1)]\n",
    "    return state\n",
    "\n",
    "def cache_write(state: RAGState) -> RAGState:\n",
    "    q = state[\"normalized_question\"]\n",
    "    a = state.get(\"answer\")\n",
    "    if not q or not a:\n",
    "        return state\n",
    "\n",
    "    QA_CACHE.add_texts(\n",
    "        texts=[q],\n",
    "        metadatas=[{\n",
    "            \"answer\": a,\n",
    "            \"ts\": time.time(),\n",
    "        }]\n",
    "    )\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b72e6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== GRAPH WIRING ==============\n",
    "graph = StateGraph(RAGState)\n",
    "\n",
    "graph.add_node(\"normalize_query\", normalize_query)\n",
    "graph.add_node(\"semantic_cache_lookup\", semantic_cache_lookup)\n",
    "graph.add_node(\"respond_from_cache\", respond_from_cache)\n",
    "graph.add_node(\"retrieve\", retrieve)\n",
    "graph.add_node(\"generate\", generate)\n",
    "graph.add_node(\"cache_write\", cache_write)\n",
    "\n",
    "graph.set_entry_point(\"normalize_query\")\n",
    "graph.add_edge(\"normalize_query\", \"semantic_cache_lookup\")\n",
    "\n",
    "def _branch(state: RAGState) -> str:\n",
    "    return \"respond_from_cache\" if state.get(\"cache_hit\") else \"retrieve\"\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"semantic_cache_lookup\",\n",
    "    _branch,\n",
    "    {\n",
    "        \"respond_from_cache\": \"respond_from_cache\",\n",
    "        \"retrieve\": \"retrieve\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"respond_from_cache\", END)\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", \"cache_write\")\n",
    "graph.add_edge(\"cache_write\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = graph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1b766fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langgraph.graph.state.CompiledStateGraph object at 0x3135aad70>\n"
     ]
    }
   ],
   "source": [
    "print(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04c66118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```mermaid\n",
       "---\n",
       "config:\n",
       "  flowchart:\n",
       "    curve: linear\n",
       "---\n",
       "graph TD;\n",
       "\t__start__([<p>__start__</p>]):::first\n",
       "\tnormalize_query(normalize_query)\n",
       "\tsemantic_cache_lookup(semantic_cache_lookup)\n",
       "\trespond_from_cache(respond_from_cache)\n",
       "\tretrieve(retrieve)\n",
       "\tgenerate(generate)\n",
       "\tcache_write(cache_write)\n",
       "\t__end__([<p>__end__</p>]):::last\n",
       "\t__start__ --> normalize_query;\n",
       "\tgenerate --> cache_write;\n",
       "\tnormalize_query --> semantic_cache_lookup;\n",
       "\tretrieve --> generate;\n",
       "\tsemantic_cache_lookup -.-> respond_from_cache;\n",
       "\tsemantic_cache_lookup -.-> retrieve;\n",
       "\tcache_write --> __end__;\n",
       "\trespond_from_cache --> __end__;\n",
       "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
       "\tclassDef first fill-opacity:0\n",
       "\tclassDef last fill:#bfb6fc\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"```mermaid\\n{app.get_graph(xray=True).draw_mermaid()}\\n```\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3ad2c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Agent memory refers to the mechanisms by which an autonomous agent, powered by a large language model (LLM), retains and utilizes information from past experiences to inform its future actions. It encompasses both short-term and long-term memory systems. \n",
      "\n",
      "1. **Short-term Memory**: This involves in-context learning, where the agent utilizes immediate information to perform tasks effectively.\n",
      "\n",
      "2. **Long-term Memory**: This allows the agent to store and recall information over extended periods, often using an external database for comprehensive records of experiences. It includes a memory stream that logs observations and events, which can be retrieved based on recency, relevance, and importance to guide the agent's behavior [doc-3].\n",
      "\n",
      "Additionally, the agent employs a reflection mechanism to synthesize memories into higher-level inferences, which helps in planning and reacting to new situations [doc-2].\n",
      "Citations: ['[doc-1]', '[doc-2]', '[doc-3]', '[doc-4]']\n",
      "Cache hit?: False\n"
     ]
    }
   ],
   "source": [
    "# ================= DEMO ===================\n",
    "if __name__ == \"__main__\":\n",
    "    thread_cfg = {\"configurable\": {\"thread_id\": \"demo-user-1\"}}\n",
    "\n",
    "    q1 = \"what is agent memory ?\"\n",
    "    out1 = app.invoke({\"question\": q1, \"context_docs\": [], \"citations\": []}, thread_cfg)\n",
    "    print(\"Answer:\", out1[\"answer\"])\n",
    "    print(\"Citations:\", out1.get(\"citations\"))\n",
    "    print(\"Cache hit?:\", out1.get(\"cache_hit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11b132d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Agent memory refers to the mechanisms by which an autonomous agent, powered by a large language model (LLM), retains and utilizes information from past experiences to inform its future actions. It encompasses both short-term and long-term memory systems. \n",
      "\n",
      "1. **Short-term Memory**: This involves in-context learning, where the agent utilizes immediate information to perform tasks effectively.\n",
      "\n",
      "2. **Long-term Memory**: This allows the agent to store and recall information over extended periods, often using an external database for comprehensive records of experiences. It includes a memory stream that logs observations and events, which can be retrieved based on recency, relevance, and importance to guide the agent's behavior [doc-3].\n",
      "\n",
      "Additionally, the agent employs a reflection mechanism to synthesize memories into higher-level inferences, which helps in planning and reacting to new situations [doc-2].\n",
      "Citations: ['(cache)']\n",
      "Cache hit?: True\n"
     ]
    }
   ],
   "source": [
    "q1 = \"Explain about agent memory ?\"\n",
    "out1 = app.invoke({\"question\": q1, \"context_docs\": [], \"citations\": []}, thread_cfg)\n",
    "print(\"Answer:\", out1[\"answer\"])\n",
    "print(\"Citations:\", out1.get(\"citations\"))\n",
    "print(\"Cache hit?:\", out1.get(\"cache_hit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fef07e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Agent memory refers to the mechanisms by which an autonomous agent, powered by a large language model (LLM), retains and utilizes information from past experiences to inform its future actions. It encompasses both short-term and long-term memory systems. \n",
      "\n",
      "1. **Short-term Memory**: This involves in-context learning, where the agent utilizes immediate information to perform tasks effectively.\n",
      "\n",
      "2. **Long-term Memory**: This allows the agent to store and recall information over extended periods, often using an external database for comprehensive records of experiences. It includes a memory stream that logs observations and events, which can be retrieved based on recency, relevance, and importance to guide the agent's behavior [doc-3].\n",
      "\n",
      "Additionally, the agent employs a reflection mechanism to synthesize memories into higher-level inferences, which helps in planning and reacting to new situations [doc-2].\n",
      "Citations: ['(cache)']\n",
      "Cache hit?: True\n"
     ]
    }
   ],
   "source": [
    "q1 = \"what is agent memory ?\"\n",
    "out1 = app.invoke({\"question\": q1, \"context_docs\": [], \"citations\": []}, thread_cfg)\n",
    "print(\"Answer:\", out1[\"answer\"])\n",
    "print(\"Citations:\", out1.get(\"citations\"))\n",
    "print(\"Cache hit?:\", out1.get(\"cache_hit\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
